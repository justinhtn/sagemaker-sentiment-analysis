{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sentiment Analysis \n","At the end of this notebook we will have made a simple site which a user can use to enter a movie review. The site will then send the review off to our deployed model which will predict the sentiment of the submitted review."]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import *\n","\n","import re\n","from bs4 import BeautifulSoup\n","\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","\n","import sagemaker\n","from sagemaker.pytorch import PyTorch\n","from sagemaker.predictor import RealTimePredictor\n","from sagemaker.pytorch import PyTorchModel"]},{"cell_type":"markdown","metadata":{},"source":["## Getting the data\n","We'll be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--2020-06-21 20:48:20--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘../data/aclImdb_v1.tar.gz’\n","\n","../data/aclImdb_v1. 100%[===================>]  80.23M  1.92MB/s    in 14s     \n","\n","2020-06-21 20:48:34 (5.70 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"]}],"source":["%mkdir ../data\n","!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def read_imdb_data(data_dir='../data/aclImdb'):\n","    '''\n","    Need to add function information here later\n","    '''\n","    data = {}\n","    labels = {}\n","    \n","    for data_type in ['train', 'test']:\n","        data[data_type] = {}\n","        labels[data_type] = {}\n","        \n","        for sentiment in ['pos', 'neg']:\n","            data[data_type][sentiment] = []\n","            labels[data_type][sentiment] = []\n","            \n","            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n","            files = glob.glob(path)\n","            \n","            for f in files:\n","                with open(f) as review:\n","                    data[data_type][sentiment].append(review.read())\n","                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n","                    \n","            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n","                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n","                \n","    return data, labels"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["data, labels = read_imdb_data()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"]}],"source":["print(f\"IMDB reviews: train = {len(data['train']['pos'])} pos / {len(data['train']['neg'])} neg, test = {len(data['test']['pos'])} pos / {len(data['test']['neg'])} neg\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def prepare_imdb_data(data, labels):\n","    \"\"\"\n","    Need to add function information here later\n","    \"\"\"\n","    \n","    data_train = data['train']['pos'] + data['train']['neg']\n","    data_test = data['test']['pos'] + data['test']['neg']\n","    labels_train = labels['train']['pos'] + labels['train']['neg']\n","    labels_test = labels['test']['pos'] + labels['test']['neg']\n","    \n","    data_train, labels_train = shuffle(data_train, labels_train)\n","    data_test, labels_test = shuffle(data_test, labels_test)\n","    \n","    return data_train, data_test, labels_train, labels_test"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IMDb reviews (total): train = 25000, test = 25000\n"]}],"source":["print(f\"IMDb reviews (total): train = {len(train_X)}, test = {len(test_X)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Taking a peek at the training data"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Review: I like bad movies. I like to rent bad movies with my friends and rip on them for their duration. Then there are abhorrent movies like this. Redline is not just a bad movie, but a telling sign that maybe the American movie industry should please, for the sake of the viewer, at least proofread scripts before funding a movie.<br /><br />If a stereotype took a crap, this movie would spawn from that. The storyline is unbearable, and the acting all around is laughable. Nadia Bjorlin and Eddie Griffin have, perhaps, the worst screen chemistry I've seen in a good while, and even individually they should be isolated from humanity and beaten with a bag of oranges until they change their profession to street merchants (about the only thing they can legitimately qualify for). Furthermore, how Angus Macfadyen got convinced to do this movie is so far beyond me that I can't even think of an analogy. I am a loyal fan of his, but this has made me question him.<br /><br />To sum it up. Several people want revenge for different reasons (and if you care enough to know what they are, you're a bigger person than me), so much so that it turns to violence (I guess). The movie is like Ouroboros, the snake that swallows its own tail, in that it's an endless cycle of confusion and dialogue not fit for human ears. This movie is essentially one big car commercial for the first half, and an indecipherable action movie for the rest, it should be avoided at any and all costs.<br /><br />I wish I could find one positive aspect to this movie, and I think it lies in the fact that eventually the credits do roll.<br /><br />P.S. Nadia Bjorlin, if that was YOU singing those two songs in this movie, then you are a hack, and I hope old age ravages you.<br /><br />P.S.S. If you DO rent this movie looking for a laughable experience, listen for the lyrics to Nadia Bjorlin's awesome songs.\n","Rating: 0\n"]}],"source":["print(f\"Review: {train_X[100]}\")\n","print(f\"Rating: {train_y[100]}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Removing html tags and tokenizing text"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def review_to_words(review):\n","    \"\"\"\n","    Need to add function information here later\n","    \"\"\"\n","    \n","    nltk.download(\"stopwords\", quiet=True)\n","    stemmer = PorterStemmer()\n","    \n","    text = BeautifulSoup(review, \"html.parser\").get_text() \n","    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) \n","    words = text.split()\n","    words = [w for w in words if w not in stopwords.words(\"english\")] \n","    words = [PorterStemmer().stem(w) for w in words] \n","    \n","    return words"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["['stefan',\n"," 'x',\n"," 'con',\n"," 'five',\n"," 'year',\n"," 'ago',\n"," 'got',\n"," 'marri',\n"," 'mari',\n"," 'marriag']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# double checking our function works on an example review\n","sample_review_output = review_to_words(train_X[0])\n","sample_review_output[:10]"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# setting cache directory incase running takes too long\n","cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\") \n","os.makedirs(cache_dir, exist_ok=True) \n","\n","def preprocess_data(data_train, data_test, labels_train, labels_test,\n","                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n","    \"\"\"\n","    Convert each review to words; read from cache if available.\n","    \"\"\"\n","\n","    # if cache_file is not None, try to read from it first\n","    cache_data = None\n","    if cache_file is not None:\n","        try:\n","            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n","                cache_data = pickle.load(f)\n","            print(\"Read preprocessed data from cache file:\", cache_file)\n","        except:\n","            pass  # unable to read from cache, but that's okay\n","    \n","    # if cache is missing, then do the heavy lifting\n","    if cache_data is None:\n","        # preprocess training and test data to obtain words for each review\n","        words_train = [review_to_words(review) for review in data_train]\n","        words_test = [review_to_words(review) for review in data_test]\n","        \n","        # writing to cache file for future runs\n","        if cache_file is not None:\n","            cache_data = dict(words_train=words_train, words_test=words_test,\n","                              labels_train=labels_train, labels_test=labels_test)\n","            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n","                pickle.dump(cache_data, f)\n","            print(\"Wrote preprocessed data to cache file:\", cache_file)\n","    else:\n","        # unpack data loaded from cache file\n","        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n","                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n","    \n","    return words_train, words_test, labels_train, labels_test"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wrote preprocessed data to cache file: preprocessed_data.pkl\n"]}],"source":["# reading in preprocessed data \n","train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"]},{"cell_type":"markdown","metadata":{},"source":["## Building word dictionary"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def build_dict(data, vocab_size = 5000):\n","    \"\"\"\n","    Need to add function information here later\n","    \"\"\"\n","    \n","    word_count = {}\n","    \n","    for review in data:\n","        for word in review:\n","            if word in word_count:\n","                word_count[word] += 1\n","            else:\n","                word_count[word] = 1\n","    \n","    sorted_words = [item[0] for item in sorted(word_count.items(), key = lambda x:x[1], reverse=True)]\n","    \n","    word_dict = {} \n","    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # -2 is so that we save room for the 'no word'\n","        word_dict[word] = idx + 2                              \n","        \n","    return word_dict"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["word_dict = build_dict(train_X)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["['movi', 'film', 'one', 'like', 'time']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# examining top words in dictionary\n","list(word_dict.keys())[:5]"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# saving word dict for later\n","data_dir = '../data/pytorch' \n","if not os.path.exists(data_dir):\n","    os.makedirs(data_dir)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# opening up saved dict in the event it has been changed\n","with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n","    pickle.dump(word_dict, f)"]},{"cell_type":"markdown","metadata":{},"source":["## Transforming reviews"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def convert_and_pad(word_dict, sentence, pad=500):\n","    NOWORD = 0 \n","    INFREQ = 1\n","    \n","    working_sentence = [NOWORD] * pad\n","    \n","    for word_index, word in enumerate(sentence[:pad]):\n","        if word in word_dict:\n","            working_sentence[word_index] = word_dict[word]\n","        else:\n","            working_sentence[word_index] = INFREQ\n","            \n","    return working_sentence, min(len(sentence), pad)\n","\n","def convert_and_pad_data(word_dict, data, pad=500):\n","    result = []\n","    lengths = []\n","    \n","    for sentence in data:\n","        converted, leng = convert_and_pad(word_dict, sentence, pad)\n","        result.append(converted)\n","        lengths.append(leng)\n","        \n","    return np.array(result), np.array(lengths)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# pulling out converted training data\n","train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n","test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["array([   5,   24,    2,    5,  443,   24,    2,  132,  943, 4826,    1,\n","          2,    5,    1,   24,    2,  133, 1061,  203,  183,    2, 1181,\n","        444, 1842,  256,  141,    1,  125, 3150,    2,  901,  493,  522,\n","          2,   15, 4827,  682, 2774,   32,  110, 1057,    1,    1, 1448,\n","          1,  332,  174,  166, 1084,   47,    7,   14, 1267, 2547,  220,\n","       3089, 2345, 3763,  255, 3816,  577,    1,   35, 4733, 2971, 3445,\n","          1,    1,  111,  548,    2,  151,  652,   14,   30,    1, 3541,\n","        123,   34,  416, 1634,  335,   23,   50,  981,  148,  134,  246,\n","        119,   37, 1773,  127,   21,   95,  498,  355,    2,    5,    1,\n","       2380, 3792, 3680, 1967, 4676,  585,  338,  651,  220, 2156,    2,\n","       1227,    4,  116,  378, 1311,   28,  252,    1,  104,    2,  302,\n","        564, 1157,  426,   36,   61,    4,  656,  660,    2,   30,  875,\n","        100,  715,  428,  837, 1440,    1,    1,  594,   42,  282,    2,\n","       2856,  189,   72,  321,    1, 1440,  443,    2,   19, 1057,  365,\n","        891, 2557,    1,    1, 1044,  282,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# making sure reviews have been processed correctly\n","train_X[100]"]},{"cell_type":"markdown","metadata":{},"source":["## Sending data to s3"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# first let's save locally to a csv\n","pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n","        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# now let's send to default sagemaker bucket in s3\n","sagemaker_session = sagemaker.Session()\n","\n","bucket = sagemaker_session.default_bucket()\n","prefix = 'sagemaker/sentiment_rnn'\n","\n","role = sagemaker.get_execution_role()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"]},{"cell_type":"markdown","metadata":{},"source":["## Build and train Pytorch model"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["estimator = PyTorch(entry_point=\"train.py\",\n","                    source_dir=\"../train\",\n","                    role=role,\n","                    framework_version='0.4.0',\n","                    train_instance_count=1,\n","                    train_instance_type='ml.p2.xlarge',\n","                    hyperparameters={\n","                        'epochs': 10,\n","                        'hidden_dim': 200,\n","                    })"]},{"cell_type":"code","execution_count":57,"metadata":{"scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n","'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n","'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"]},{"name":"stdout","output_type":"stream","text":["2020-06-21 22:40:06 Starting - Starting the training job...\n","2020-06-21 22:40:08 Starting - Launching requested ML instances.........\n","2020-06-21 22:41:39 Starting - Preparing the instances for training.........\n","2020-06-21 22:43:09 Downloading - Downloading input data...\n","2020-06-21 22:43:46 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n","\u001b[34mbash: no job control in this shell\u001b[0m\n","\u001b[34m2020-06-21 22:44:16,897 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n","\u001b[34m2020-06-21 22:44:16,922 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n","\u001b[34m2020-06-21 22:44:19,939 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n","\u001b[34m2020-06-21 22:44:20,164 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n","\u001b[34mGenerating setup.py\u001b[0m\n","\u001b[34m2020-06-21 22:44:20,165 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n","\u001b[34m2020-06-21 22:44:20,165 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n","\u001b[34m2020-06-21 22:44:20,165 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n","\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n","\u001b[34mProcessing /opt/ml/code\u001b[0m\n","\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n","  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n","\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n","\n","2020-06-21 22:44:16 Training - Training image download completed. Training in progress.\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n","\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\n","\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n","  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n","\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n","  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n","\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n","  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n","\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n","\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n","\u001b[34m  Downloading https://files.pythonhosted.org/packages/b8/7b/01510a6229c2176425bda54d15fba05a4b3df169b87265b008480261d2f9/regex-2020.6.8.tar.gz (690kB)\u001b[0m\n","\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n","  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\u001b[0m\n","\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n","  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n","\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n","  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n","\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n","\u001b[34mBuilding wheels for collected packages: nltk, train, regex\n","  Running setup.py bdist_wheel for nltk: started\u001b[0m\n","\u001b[34m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n","  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n","  Running setup.py bdist_wheel for train: started\u001b[0m\n","\u001b[34m  Running setup.py bdist_wheel for train: finished with status 'done'\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-g_eh24wd/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n","  Running setup.py bdist_wheel for regex: started\u001b[0m\n","\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n","  Stored in directory: /root/.cache/pip/wheels/9c/e2/cf/246ad8c87bcdf3cba1ec95fa89bc205c9037aa8f4d2e26fdad\u001b[0m\n","\u001b[34mSuccessfully built nltk train regex\u001b[0m\n","\u001b[34mInstalling collected packages: pytz, numpy, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\u001b[0m\n","\u001b[34m  Found existing installation: numpy 1.15.4\n","    Uninstalling numpy-1.15.4:\n","      Successfully uninstalled numpy-1.15.4\u001b[0m\n","\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 html5lib-1.0.1 joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.1 regex-2020.6.8 soupsieve-2.0.1 tqdm-4.46.1 train-1.0.0 webencodings-0.5.1\u001b[0m\n","\u001b[34mYou are using pip version 18.1, however version 20.2b1 is available.\u001b[0m\n","\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","\u001b[34m2020-06-21 22:44:43,395 sagemaker-containers INFO     Invoking user script\n","\u001b[0m\n","\u001b[34mTraining Env:\n","\u001b[0m\n","\u001b[34m{\n","    \"output_data_dir\": \"/opt/ml/output/data\",\n","    \"log_level\": 20,\n","    \"resource_config\": {\n","        \"current_host\": \"algo-1\",\n","        \"network_interface_name\": \"eth0\",\n","        \"hosts\": [\n","            \"algo-1\"\n","        ]\n","    },\n","    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n","    \"hosts\": [\n","        \"algo-1\"\n","    ],\n","    \"hyperparameters\": {\n","        \"hidden_dim\": 200,\n","        \"epochs\": 10\n","    },\n","    \"module_dir\": \"s3://sagemaker-us-east-2-592944050271/sagemaker-pytorch-2020-06-21-22-40-06-479/source/sourcedir.tar.gz\",\n","    \"num_gpus\": 1,\n","    \"user_entry_point\": \"train.py\",\n","    \"input_dir\": \"/opt/ml/input\",\n","    \"current_host\": \"algo-1\",\n","    \"model_dir\": \"/opt/ml/model\",\n","    \"additional_framework_parameters\": {},\n","    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n","    \"module_name\": \"train\",\n","    \"input_config_dir\": \"/opt/ml/input/config\",\n","    \"job_name\": \"sagemaker-pytorch-2020-06-21-22-40-06-479\",\n","    \"network_interface_name\": \"eth0\",\n","    \"output_dir\": \"/opt/ml/output\",\n","    \"num_cpus\": 4,\n","    \"channel_input_dirs\": {\n","        \"training\": \"/opt/ml/input/data/training\"\n","    },\n","    \"input_data_config\": {\n","        \"training\": {\n","            \"S3DistributionType\": \"FullyReplicated\",\n","            \"TrainingInputMode\": \"File\",\n","            \"RecordWrapperType\": \"None\"\n","        }\n","    }\u001b[0m\n","\u001b[34m}\n","\u001b[0m\n","\u001b[34mEnvironment variables:\n","\u001b[0m\n","\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-06-21-22-40-06-479\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-592944050271/sagemaker-pytorch-2020-06-21-22-40-06-479/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n","\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n","\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n","\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n","\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n","\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n","\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n","\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n","\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-592944050271/sagemaker-pytorch-2020-06-21-22-40-06-479/source/sourcedir.tar.gz\u001b[0m\n","\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n","\u001b[34mSM_MODULE_NAME=train\u001b[0m\n","\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n","\u001b[34mSM_NUM_GPUS=1\u001b[0m\n","\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n","\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n","\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n","\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n","\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n","\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n","\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n","\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n","\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n","\u001b[34mSM_NUM_CPUS=4\u001b[0m\n","\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n","\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n","\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n","\u001b[34mSM_CHANNELS=[\"training\"]\n","\u001b[0m\n","\u001b[34mInvoking script with the following command:\n","\u001b[0m\n","\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n","\n","\u001b[0m\n","\u001b[34mUsing device cuda.\u001b[0m\n","\u001b[34mGet train data loader.\u001b[0m\n","\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34mEpoch: 1, BCELoss: 0.6638028390553533\u001b[0m\n","\u001b[34mEpoch: 2, BCELoss: 0.5903471173072348\u001b[0m\n","\u001b[34mEpoch: 3, BCELoss: 0.5269349436370694\u001b[0m\n","\u001b[34mEpoch: 4, BCELoss: 0.442237063330047\u001b[0m\n","\u001b[34mEpoch: 5, BCELoss: 0.40509845833389124\u001b[0m\n","\u001b[34mEpoch: 6, BCELoss: 0.3793247591476051\u001b[0m\n","\u001b[34mEpoch: 7, BCELoss: 0.3388176852343034\u001b[0m\n","\u001b[34mEpoch: 8, BCELoss: 0.31961560766307673\u001b[0m\n","\u001b[34mEpoch: 9, BCELoss: 0.3105925583109564\u001b[0m\n","\u001b[34mEpoch: 10, BCELoss: 0.28699194959231783\u001b[0m\n","\u001b[34m2020-06-21 22:47:41,273 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n","\n","2020-06-21 22:47:50 Uploading - Uploading generated training model\n","2020-06-21 22:47:50 Completed - Training job completed\n","Training seconds: 281\n","Billable seconds: 281\n"]}],"source":["estimator.fit({'training': input_data})"]},{"cell_type":"markdown","metadata":{},"source":["## Testing model\n","We'll deploy our model first to also test to make sure our deployment works as expected"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n","'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"]},{"name":"stdout","output_type":"stream","text":["---------------!"]}],"source":["predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["def predict(data, rows=512):\n","    '''\n","    Need to add function information here later\n","    '''\n","    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n","    predictions = np.array([])\n","    for array in split_array:\n","        predictions = np.append(predictions, predictor.predict(array))\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# getting and rounding the prediction values \n","predictions = predict(test_X.values)\n","predictions = [round(num) for num in predictions]"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["0.85104"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["# getting our accuracy score\n","accuracy_score(test_y, predictions)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing a review"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["test_review = 'Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!'"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# running our test review through the preprocessing steps\n","test_data_review_to_words = review_to_words(test_review)\n","test_data = [np.array(convert_and_pad(word_dict, test_data_review_to_words)[0])]"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["array(0.9893678, dtype=float32)"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["# getting our prediction\n","test_prediction = predictor.predict(test_data)\n","test_prediction"]},{"cell_type":"markdown","metadata":{},"source":["Because the return value of our model is close to 1, we can be pretty sure the review was positive!"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# optional delete endpoint as to save cost\n","estimator.delete_endpoint()"]},{"cell_type":"markdown","metadata":{},"source":["## Deploying endpoint again for web app\n","Because we're going to be passing the model a string, and Pytorch expects a numpy array, we have to build a wrapper to the stringpredictor class."]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["class StringPredictor(RealTimePredictor):\n","    def __init__(self, endpoint_name, sagemaker_session):\n","        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"]}],"source":["model = PyTorchModel(model_data=estimator.model_data,\n","                     role = role,\n","                     framework_version='0.4.0',\n","                     entry_point='predict.py',\n","                     source_dir='../serve',\n","                     predictor_cls=StringPredictor)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"]},{"name":"stdout","output_type":"stream","text":["---------------!"]}],"source":["predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"]},{"cell_type":"markdown","metadata":{},"source":["### Again we should test the deployed model using a sample set from our testing data"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["def test_reviews(data_dir='../data/aclImdb', stop=250):\n","    \n","    results = []\n","    actual = []\n","     \n","    for sentiment in ['pos', 'neg']:\n","        \n","        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n","        files = glob.glob(path)\n","        \n","        files_read = 0\n","        \n","        print('Starting ', sentiment, ' files')\n","        \n","        for f in files:\n","            with open(f) as review:\n","                if sentiment == 'pos':\n","                    actual.append(1)\n","                else:\n","                    actual.append(0)\n","                # reading in the review and convert to 'utf-8' for transmission via HTTP\n","                review_input = review.read().encode('utf-8')\n","                # sending the review to the predictor and store the results\n","                results.append(float(predictor.predict(review_input)))\n","                \n","                \n","            # we'll only send 250 reviews since this is just a test\n","            files_read += 1\n","            if files_read == stop:\n","                break\n","            \n","    return actual, results"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting  pos  files\n","Starting  neg  files\n"]}],"source":["# grabbing the actual sentiment values and the predictions from our test review function\n","actual, results = test_reviews()"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/plain":["0.85"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["# getting our accuracy score. Isn't sklearn so handy?!\n","accuracy_score(actual, results)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/plain":["b'1.0'"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["# let's also send the test review from earlier since we already know the sentiment as a test\n","predictor.predict(test_review)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["'sagemaker-pytorch-2020-06-22-00-52-31-245'"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["predictor.endpoint"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"conda_pytorch_p36","language":"python","name":"conda_pytorch_p36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"deepnote_notebook_id":"bf6ea971-3a85-4546-a3b6-4dc132162c6d"},"nbformat":4,"nbformat_minor":4}